# Natural Language Processing(RNN)

생성일: 2023년 4월 26일 오후 2:34

RNN 계열의 모델

- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드에 다음 계산의 입력으로보내는 특징을 갖고 있음

![Untitled](image\RNN1.png)

- 병렬 처리가 불가능하고 그로인해 속도가 느리다는 단점이 있음

## seq-to-seq 언어 모델

- 번역기에서 대표적으로 사용되는 모델
- 문장의 단어들을 순차적으로 입력받아 하나의 정보로 압축해서 하나의 백터로 만든다
- context vector라고 부르며, 인코더는 이를 디코더에 전송하고 결과를 하나씩 출력한다.

![https://wikidocs.net/images/page/24996/seq2seq모델11.PNG](https://wikidocs.net/images/page/24996/seq2seq모델11.PNG)

- 하나의 고정된 크기의 벡터에 모든 정보를 압축하기 때문에 손실이 발생한다.
- RNN의 문제인 기울기 소실문제가 존재한다.

## Attention

어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점 마다 인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점이다

해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 보게 된다.

## Transformer

- 어텐션을 RNN계을의 모델 보정이 아닌 어텐션만으로 인코더와 디코더를 만든 것이다.
- 트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀸스를 입력받고 디코더에서 출력 시퀸스를 출력하는 방식이다.
- 이전에는 각각 하나의 RNN이 t개의 시점을 가지는 구조였다면, 이번에는 인코더와 디코더라는 단위가 N개의 구성되는 구조이다.

![https://wikidocs.net/images/page/31379/transformer2.PNG](https://wikidocs.net/images/page/31379/transformer2.PNG)

- seq2seq처럼 <sos>를 입력 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행한다.

### Positional Encoding

- RNN이 자연어 처리에 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보 가질 수 있다는 점이 었다.
- 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하느데, 이를 포지셔널 인코딩(positional encoding)이라고 한다.