# 2023_03_28


# 랜덤 포레스트(Random Forest)

- 다수의 결정  트리(decision tree)로 구성된 앙상블 학습(ensemble learning)알고리즘이다.

**앙상블** : 여러가지 모델을 사용하여 정확도를 개선하는 방법

## 랜덤 포레스트 구성방식

1. 훈련 데이터에서 무작위로 추출된 샘플(bootstrap samplet)로 결정 트리를 학습한다.
2. 1번 과정을 반복하여 많은 수의 결정 트리를 생성한다.
3. 결정 트리는 데이터의 무작위 subset를 사용하여 학습, 결정트리들은 서로 다른 모양을 가진다. 이러한 다양성은 앙상블의 평향을 줄이는데 도움이 된다,
4. 새로운 입력에 대한 예측 각 결정 트리의 출력을 종합하여 만들어짐

## 랜덤 포레스트 : 의사결정나무의 앙상블

- 다수의 의사결정 나무들의 결과로 부터 모델을 생성
- 모델 생성에 다양성과 임의성을 부여
- 모델의 정확도를 높이고 과적합 발생 가능성을 낮춤
- 올바른 예측은 강화하고, 잘못된 예측은 상쇄하는 경화 존재

### 다양성(diversity) - 배깅(Bagging)

- Bagging = Bootstrap + Aggregating
- 주어진 Data를 사용하여 여러 개의 서로 다른 Train Data 생성
    - 생성된 Train Data 마다 별도의 의사결정 나무 생성
    - Hyperparameter(n_estimators)로 의사결정 나무 생성 지정

### 개별 Train Data는 Bootstrap 방식으로 생성

- Bootstrap Data는 Original Data에서 단순한 복원 임의추출법으로 생성

### 임의성(Random) - Random subspace

- 의사결정 나무 생성 시 변수 무작위 선택
- 무작위로 변수를 추출하여 적용
- 무작위 변수 개수를 1  ~전체 변수의 개수 사이에 지정
- Hyperparameter : max_feature
    - 기본값 : sqrt(변수의 개수())

## 랜덤포레스트의 이점

- 높은 정확도 : 많은 수의 결정트리가 앙상블을 구성하므로 예측 성능이 높다.
- 높은 내결함성 : 랜덤 서브세트를 사용함으로 과대 적합을 방지한다.
- 특징의 중요도 평가 : 각 특징이 예측에 얼마나 중요한지 평가
- 병령화 기능 : 결정 트리를 병렬로 학습할 수 있다.
- 해석 가능성 : 결정 트리는 인간이 이해하기 쉬운 모델이다. 랜덤 포레스트 모델의 해석도 가능하다

## 교차 검증 (Cross - Validation)

- Overfitting을 방지하기위하여 수행
- Validation을 한번만 수행하면 특정 Data에만 최적화 될 수 있음
- 다항하게 training Data를 validataion Data를 변경하면서 모델 평가 수행

## K-Fold Cross Validation

- Training Data를 균들하게 K개의 그룹으로 나누어서 검증ㅏ
- (K ㅡ1) 개의 Training Fold 와 1개의 validation Fold를 걱정
- Data가 충분히 많다면 K-Fold Cross- validation
- 데이터가 매우 적다면 데이터의 개수만큼 교차 검증을 수행(Loocv)