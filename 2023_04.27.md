# Natural Language Processing

생성일: 2023년 4월 19일 오후 4:46

## NPL 개요

- 학습된 모델로 문장 분류, 문장 요약, 문장 생성, 번역 등의 작업 수행
- 대량의 말뭉치를 딥러닝 모델 학습에 활용
    - 말충치(Corpus) : 모데링을 위하여 특정 목적을 가지고 수집한 언어의 표본
- 자연어 처리의 목적은 이해가 아님
    - 연산이나 처리의 영역
- 수학적 연산을 위하여 자연어를 숫자로 변환하는 작업이 필요
- 인간이 사용하는 자연어를 컴퓨터가 연산할 수 있는 벡터로 변환
    - Vector : 숫자의 나열
        
        

## Processing

- 자연어 학습을 위하여 수집된 데이터에 대한 전처리 작업 필요
- Tokenization
    - Sentence, word, Character : 문장을 분할하여 나눔
    - Stop word(불용어)
        - 문장에서 제거해도 의미에 큰 영향을 미치지 않는 단어
        - “그”,  “저” , “The”, “a”와 같은 단어가 불용어에 해당
        - 원하는 목적에 따라 변할 수 있다.
    - Stemming, Lemmatiation
        - 주로 영어에서 쓰이는 방법론
        - Stemming : 단어에서 접사 등을 제거하여 어근을 추출하는 것으로, 문서 내 단어의 빈도수 계산 등에 활용된다.
        - Lemmatization : 단어의 원형을 추출하는 것으로, 문서 내 단어의 의미를 파악하는 데 활용된다.
    - 한국어에서는 주로 형태소 분석을 통해 언근 추출, 품사 태깅, 불용어 처리등의 작업을 함
- Encoding(Vectorization)
    - 정수 인코딩(Integer Encoding), 원-한 인코딩(One-Hot Encoding)

## Tokenization

- 수집한 말뭉치(Corpus)를 토큰(Token) 단위로 나누는 작업
    - 토큰 단위는 의미를 가지는 크기로 정의
- 일반적으로 형태소(Morpheme) 단위의 토큰화 수행
    - 형태소란 의미를 가지는 가장 작은 말의 단위를 의미
- 단어 토큰화(Word Tokenization) : 단위를 기준으로 나눈 것
- 문장 토큰화(Sentence Tokenization) : 토큰의 단위가 문장인 경우
- 한국어의 경우 조사와 띄어쓰기 등으로 영어보다 토큰화가 어려움

## Encoding(Vectorization)

- 정수 인코딩(Integer Encoding)
    - 단어를 고유한 정수로 맵핑(Mapping)하는 방식
    - 단위가 2000개라면 각각 단어에 고유한 정수를 인덱스로 부여
- 원 - 핫 인코딩(One-Hot Encoding)
    - 단어집합 크기의 벡터 차원으로 0과 1을 사용하여 표현
    - 이러한 벡터를 원-핫 벡터(One-Hot-Vector)라고 함

## Language Model

- 언어(단어, 문장)에 존재하는 특징을 표현하기 위해 확률을 할당하는 것
- 문장(Sequence)이 적절한지 , 말이 되는지 판단하기 위한 기준
    - P(승객이 버스에 탔다) vs. P(승객이 버스에 태운다)
    - 나는 딥러닝을 ~P(배운다) vs. P(어렵다) vs. P(고친다) vs. P(가르친다)
- Bow(Bag of Words)
- n - gram
- TF-IDF(Term Frequency - Inverse Document Frequency)

# Bag of Words

- 문장이 가지는 모든 단어(Word)를 문맥이나 순서를 무시하고 문장에 포함된 단어의 빈도를 기반으로 특징을 추출
- 발생 빈도가 높을 수록 중요한 단어로 인식
- Bow - Feature Vectorization
    - M개의 문장(Sentence) 또는 문서(Document)
    - 모든 단어(Term) 추출 시 N 종류의 단어 존재
    - M x N 크기의 행렬(Term -Document Matrix)생성
- **Bow 예시**
    - A : I like dog
    - B : You like dog
    - C : I hate bug

|  | I | like | dog | You | hate | bug |
| --- | --- | --- | --- | --- | --- | --- |
| A | 1 | 1 | 1 | 0 | 0 | 0 |
| B | 0 | 1 | 1 | 1 | 0 | 0 |
| C | 1 | 0 | 0 | 0 | 1 | 1 |
- 장점
    - 쉽고 빠른 구현 가능
    - 문맥을 고려하지 않고 단어의 출현 빈도만 고려하므로, 특정 분야에서는 좋은 성능을 보임
    - 단어의 빈도를 기반으로 해기 때문에, 단어의 중요도가 높아져, 문서의 핵심 단어를 잘 반영한다
- 단점
    - 언어의 특성상 자주 등장하는 단어에 높은 중요도를 부여
    - 단어의 순서를 고려하지 않아, 문맥 의미(Semantic Context) 반영 부족
    - 희소 행렬(Sparse Metrix)을 생성하여 학습시간 및 성능에 부정적인 역향
    - 중요한 단어와 불필요한 단어의 구분이 어려움

## n-gram

- BoW의 단어 순서를 무시하는 단점을 보완
    - 1-gram(Unigram), 2-gram(Bigram), 3-gram(Trigram)
- 연속된 n개의 단어를 하나의 토큰으로 추출하는 방법
- 문장에서 의미 있는 구, 문맥을 파악할 때 사용
- Example
    - Machine Learning is fun and is not boring
        
        
        | N-gram | Examples |
        | --- | --- |
        | Unigram (n=1) | Machine, Learning, is, fun, and, is, not, boring |
        | Bigram (n=2) | Machine Learning, Learning is, is fun, fun and, and is, is not, not boring |
        | Trigram (n=3) | Machine Learning is, Learning is fun, is fun and, fun and is, and is not, is not boring |
        | 4-gram (n=4) | Machine Learning is fun, Learning is fun and, is fun and is, fun and is not, and is not boring |

## Term Frequency - Inverse Document Frequency

- BoW의 단어의 빈도수만 고려하는 단점을 보안
    - 개별 문장(문서)에 자주 나타나는 단어에 높은 가중치 부여
    - 모든 문장(문서)에 전반적으로 자주 나타나는 단어에는 패널티 부여
- TF Score
    - 특정 문장(Document)에 등장한 특정 단어(Term)의 등장 횟수
    - $TF_{(t,d)} = \frac{Count_{(t,d)}}{\sum_{(t' \in d)}Count_{(t',d)}}$
- **IDF Score**
    - 특정 단어(Term)가 등장한 문장(Document)의 수
    - $IDF_{(t)} = log(\frac{N}{df_{(t)}})$
        - $N$: 전체 문장의 수
        - $df_{(t)}$: 단어 $t$가 등장한 문장의 수
        - $IDF_{(t)}$: 단어 $t$의 Inverse Document Frequency

## Similarity

- 단어나 문장 간 유사도(Similarity)를 비교
    - 단어나 문장을 벡터로 변환 후 유사도 비교
- Euclidean Distance
- Cosine

### Euclidean Distance Similarity

- 벡터 간의 거리를 계산하여 유사도를 측정
- (1,5)와 (5,2) 사이의 유클리드 거리는 $ed1 = \sqrt{(5-1)^2+(2-5)^2}$
- (5,1)와 (5,2) 사이의 유클리드 거리는 $ed2 = \sqrt{(5-5)^2+(2-1)^2}$
- ed1, ed2는 각 좌표간의 유사도를 나타낸다.
- 거리가 짧을 수록 유사도가 높다.

### Cosine Simil

- 두 벡터 간의 사잇각을 계산하여 유사도를 측정한다
- $Cos\theta = \frac{\textbf{A} \cdot \textbf{B}}{\lVert\textbf{A}\rVert \cdot \lVert\textbf{B}\rVert}$사
- 사이각이 작을 수록 유사도 높음
- 벡터의 크기가 아닌 방향성 기반

## Embedding

- Embedding
    - 단어나 문장을 벡터로 변환 후  벡터 공간으로 ‘끼워 넣는’ 의미
- Word Embedding은 단어를 희소 표현이 아닌 밀집 표현으로 변환
- 희소 표현(Sparse Representation)
    - 대부분 값이 0으로 표현되어, 공간낭비 발생 및 단어의 의미를 담지 못함
- 밀집 표현(Dense Representation)
    - 지정된 차원의 밀집 벡터로, 실수값을 사용하여 표현

|  | One-Hot Vector  | Embedding Vector  |
| --- | --- | --- |
| 차원 | 단어 집합 크기 | 임의 지정 가능 |
| 다른 표현과의 관계 | 단어 간 유사성 반영 불가 | 단어 간 유사성 반영 가능 |
| 표현 방법 | 각 차원은 서로 독립적 | 각 차원은 서로 의미적 연관성을 가짐 |
| 저장 공간 | 많은 저장 공간 요구 | 상대적으로 적은 저장 공간 요구 |
| 차원 축소 | 효과적으로 불가능 | 차원 축소 가능 |
| 값의 타입 | 0과1 | 실수 |

## word2vec

- One-Hot Encoding 및 TF-IDF 방식의 단점 보완
- 문장 내의 비슷한 위치(neighbor words)에 있는 단어로부터 유사도 획득
    - [https://word2vec.kr](https://word2vec.kr/)
- 각각의 단어 벡터가 단어 간 유사도를 반영한 값을 가지고 있음
- 분산 표현(Distributed Representation)
    - 분포 가설 : 비슷한 위치에 등장하는 단어는 비슷한 의미를 가짐
    
    ![https://velog.velcdn.com/images%2Fyuns_u%2Fpost%2F6f41efa9-d5f0-4a98-84f4-5af569fce1ca%2Fimage.png](https://velog.velcdn.com/images%2Fyuns_u%2Fpost%2F6f41efa9-d5f0-4a98-84f4-5af569fce1ca%2Fimage.png)
    
- CBOW(Continuous Bag of Words)
    - 주변에 있는 단어를 사용하여 중간에 있는 단어를 예측하는 방법
    - 2 window : “The fat cat sat on the mat” Center Word, Context Word
    - 윈도우를 이동하여(Sliding Window)하며 생성된 데이터로 임베딩 학습
- Skip-gram
    - 중간 단어(Context Word)로부터 주변 단어(Context Word)를 예측하는 방법
    - 2 window : “The fat cat sat on the mat” Center Word, Context Word
    - 윈도우 크기만큼 주변 단어를 사용하여 생성된 데이터로 임베딩 학습