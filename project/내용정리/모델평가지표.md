# 텍스트 요약 모델 평가지표



# Rouge Score

- Recall-Oriented Understudy for Gisting Evaluation
- 텍스트 자동 요약, 기계 번역 등 자연어 생성 모델의 성능을 평가하기 위한 지표이다.
- 모델이 생성한 요약본 혹은 번역본을 사람이 미리 만들어 놓은 참조본과 대조해 성능 점수를 계산합니다
- 시스템 요약 : 모델이 생성
- 참조 요약 : 대게 사람이 직접 만든 요약

## Preicison과 Recall의 의미

- Recall
    - 참조 요약본을 구성하는 단어 중 몇 개의 단어가 시스템 요약본의 단어들과 겹치는지를 보는 점수
    - $\frac{Number\,of\,overlapped\,words} {Total\,words\,in\,references\,summary}$
    - 해당 수치만으로는 모델의 성능을 대변하지 못함
        - ex) 시스템 요약본이 매우 긴 문장이었을 경우 참조 요약본과 크게 관련이 없을지라도 참조 요약본의 단어 대부분이 포함되어 있을 가능성이 있음
- Precision
    - Recall과 반대로 모델이 생성한 시스템 요약본 중 참조 요약본과 겹치는 단어들이 얼마나 많이 존재하는지를 보고자 한다
    - $\frac{Number\,of\,overlapped\,words} {Total\,words\,in\,system\,summary}$

## **ROUGE-N**, **ROUGE-S**, **ROUGE-L**

- 요약본의 일정 부분을 비교하는 성능 지표
- ROUGE - N
    - unigram,bigram 등 시스템 요약본과 참조 요약본 간 겹치는 수를 보는 지표
- **ROUGE-L**
    - LCS 기법을 이용해 최장 길이로 매칭되는 문자열을 측정합니다. LCS의 장점은 연속적 매칭을 요구하지 않고, 문자열 내에서 발생하는 매칭을 측정하기 떄문에 보다 유연한 성능 비교가 가능하다는 것 입니다.
- ROUGE-S
    - Windows size가 주어졌을 때, Window size내에 위치하는 단어쌍들을 묶어 해당 단어쌍들이 얼마나 중복되게 나타나는 지를 측정
    - Skip-gram Co-ocurrence 기법이라고 함

# RDASS

(****Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization)****

---

- 한국어는 다양한 형태소를 여러 의미를 표현하는 단어로 결합한 응집어이기 떄문에 ROUGE는 한국어 요약본에 적합하지 않다
- ROUGE는 참고요약을 하나만 제공했을 때 상관관계가 크게 감소 했음을 증명했다.
- 개인이 문서를 수동으로 요약하는 과정을 고려할 때, ROUGE는 생성된 요약과 참조 요약 사이의 의미적 의미를 반영하지 않기 떄문에 제한적
- ROUGE 점수는 n-gram 중복에 근거해 계산되기 떄문에 동의어나 구를 설명하지 않는다.

## 차별점

- 참고요약뿐만 아니라 문서도 고려하여 생성된 요약을 평가하는 방법을 제안
- 우리의 평가 모델은 비지도 학습으로 부터 바이트 페어인코딩 토큰화 방법에 근거한 사전 훈련된 신경 네트워크를 활용하기 때문에 out fo vacbulary 문제에 견고하다

- 이해한 내용
    - 기존의 SBERT모델을 활용하여 요약검출을 참조 요약문과 원문의 벡터값 의 코싸인값과 원문의 문서의 벡터값과 시스템 요약문의 벡터값의 코싸인값
    - 두 코싸인값의 평균
    - ROUGE와 비슷하게 파이썬 패키지로 제공을 안하는것 같음

[https://velog.io/@crosstar1228/NLPRouge-score-Summarization의-평가-Metric](https://velog.io/@crosstar1228/NLPRouge-score-Summarization%EC%9D%98-%ED%8F%89%EA%B0%80-Metric)