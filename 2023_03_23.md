# 2. Cross-Entropy Error(교차 엔트로피 오차)_이진분류

생성일: 2023년 3월 21일 오후 3:10

# 2. Cross-Entropy Error(교차 엔트로피 오차)_이진분류

- 이진 분류에서 CE는 정답 레이블과 모델이 예측한 확률 분포 사이의 차이를 측정하는 loss function으로 사용된다.
- CE는 정답 레이블이 1인경우 0인 경우로 나누어 수식을 성명할 수 있다.
- 
    - $CEE =  -[ylog(\hat{y}) + (1-y)log(1-\hat{y})]$
    - y = 1인 경우
        - $CEE = -ylog(\hat{y})$
            - y : 정답 레이블(1 or 0)
            - p : 모델이 입력 x에 대해 y = 1을 예측한 확률(0과 1 사이의 값)
            - log : 밑이 2인 로그 함수
    - y = 0인 경우
        - $CEE = -ylog(1-\hat{y})$
- CEE는 모델의 예측 확률(p)이 정답 레이블(y)에 가까울수록 작아지며, 정답 레이블과 예측 확률이 매우 다른 경우 커진다.
- 모델은 CEE를 최소화하는 방향으로 학습을 진행
- 정답 레이블에 가까운 예측을 수행하도록 만듬

## 2) inforamtion Theory(정보이론)

- Information Gain(정보 이득량 척도)
    - 자주 발생하지 않는 사건은 자주발생하는 사건보다 전달하는 정보량이 많음
    - information Gain(정보 이득량)은 정보의 희귀성(발생가능성)에 반비례
- Degree of Suprise(놀람의 정도)
    - 예상하기 어려운 정보에  더 높은 가치를 매기는 것
    

### 3) Entropy

- 불확실성(Entropy)이 낮으면 분류 정확도가 높아짐
- Entropy : 불확실성의 정도
- 정보량(확률변수)의 평균(기대값)
- $Entropy =E(-log(p(x)))$
    
    $= sum(-log(p(x)) * p(x))$
    
    $= -sum(p(x) * log(p(x)))$