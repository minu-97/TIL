# TensorFlow



# TensorFlow

- 데이터 흐름 프로그램잉을 위한 Open Source Software Library
- Neural Network 같은 Machine Learning Modeling에 활용
- 구글 브레인팀에 의해 개발, 2015년 아파치 2.0 오픈소스 라이센스로 공개
- 주요 특징
    - Keras API를 활용하여 손쉬운 모델 빌드
    - 플랫폼 관계 없이 모델을 학습시키고 배포가능
    - 빠른 프로토 타입 제작과 디버깅 구현 가능

## Keras

- Python 기반의 Deep Learning FramWork(Library)
- 내부적으로 Tensorflow, Theano, CNTK 등의 Deep Learning 전용 엔징 구동
- 누구나 쉽게 Deep Learning Model 생성 기능
- Keras  사용자는 복잡한 내부 엔진에 대하여 알지 못해도 됨
- 직관적인 API를 통하여 MLP, CNN ,RBB 등의 모델 생성 가능
- 다중 입력 및 출력 구성 가능
- TensorFlow 1.14버전 부터 공식 코어 API로 추가
- 사용자 중심의 상위 레벨 인터페이스 제공
    - 하위 레벨 계산은 일반적으로 TensorFlow 사용
    - 동일한 코드를 cpu 및 다양한 GPU에서 실행 가능

### keras with GPU

- CPU : 복잡한 연산 수행에 적합
- GPU : 단순 대량 연산에 적합
    - Deep Learning Matrix 연산에 활용
    - NVIDA CUDA Neural Network Library

### Tensor

- Neural Network 학습의 기본 데이터 단위
    - 숫자 데이터를 담기 위한 컨테이너
    - 임의의 차원 또는 축을 가짐
- Tensor in NLP
    - 문장과 단어를 숫자 벡터로 매핑
        - ex) 하얀 고양이, 하얀 강아지, 하얀 비둘기
    - Unique Word Dictionary
        - ex) (Rank 2)
            - 하얀 고양이 :[[1,0,0,0],[0,1,0,0]]
            - 하얀 강아지 : [[1,0,0,0].[0,0,1,0]]
    - mini-batch Input(Rank3)
        - 하얀 고양이 하얀 강이지 하얀 비둘 ⇒ (3,2,4) : Rank 3 Tensor
- Tensor in Grayscale Image
    - (3,5,5) : Rank 3 Tensor
    - (Number of Images, Rows, Columns)
- Tensor in RGB Image
    - (3, 5, 5, 3) : Rank 4 Tensor
    - (Number of Images, Rows, Columns, RGB channel)
- Tensor in RGB Color Video
    - (3, 5 , 600, 800, 3 ) : Rank 5 Tensor
    - (Video Frames, Number of Images, Rows, Columns, RGB channel)

### Softmax Activaiton

- 소프트 맥스 함수
    - $f(xk) = \frac {exp(x_k-m)} {\sum^n_i = 1{exp(x_i-m)}}$
- 출력층에서 중 분류 수행
    - softmax() 함수의 출력값 범위 : 0 ~ 1 (확률값)
- sigmoid() vs softmax()
    - sigmoid() : 함수의 출력값이 각각 0 ~ 1 사이의 값을 가짐
    - softmax() : 전체 출력값의 합이 1이 되어야하기 때문에 학습효과가 증가
    - $f(x) = \frac {1}{1+exp(-x)}$  **vs.  $f(xk) = \frac {exp(x_k-m)} {\sum^n_i = 1{exp(x_i-m)}}$ , $m = max(x_k)$**
    - softmax()함수는 다중 분류 문제에서 사용
    - sigmoid() 함수는 이진 분류 문제에서 사용
- Categorical Classiffication
    - Categorical Cross-Etropy Error(CEE)
        - $CEE = -sum(y*log(\hat{y}))$
    - MSE vs. CEE
        - y_1 = [1,0,0], y_2 = [0, 1, 0], y_3 = [0,0,1]
        - y_hat = [0.1, 0.7, 0.2]
        - $MSE = \frac {((0 - 0.2)^2 + (1 - 0.7)^2 + (0 - 0.2)^2} {3}$
        - $CEE = -0*log(0.1) - 1*log(0.7) - 0*log(0.2)$