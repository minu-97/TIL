# Deep Neural Network

생성일: 2023년 3월 30일 오후 3:24

## keras Modeliong - Binary Classification

### 실습1

- IMDB
    - 영화에 대한 양극화된 리뷰 5만개 제공
    - 리뷰에 포함된 단어를 기준으로 긍정과 부정으로 이진 분류
    - [실습코드](MachineLearning\Deeplearning\101_DNN_IMDB_Binary_Classification_CPU.ipynb)

### 실습2

- Handwritten Digits in the MNIST Database
    - [실습코드](MachineLearning\Deeplearning\201_DNN_mnist_Categorical_Classification_Overfitting_GPU.ipynb)
    - 위의 코드의 모델을 실행결과
    
    ![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled.png)
    
    - train_loss와 validation_loss의 차이가 크게 발생
    - train_loss는 감소하지만 validation_loss는 증가한다.
    
    ![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%201.png)
    
- Overfiting
    - Train 데이터에만 최적화된 상태
        - train loss vs. Validation Loss
            1. 더 많은 train data (추가 적인 비용과 시간이 요구되기 때문에 실질적으로 활용하기 힘들다)
            2. model capacity (모델에 있는 학습 파라미터의 수)
            3. L2 Regularization
            4. Dropout
            5. Batch Normalization

### Model Capacity

- Model Capacity 감소 전략
- HIdden Layer 및 Node 개수 줄이기

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%202.png)

- 기존의 모델에서 레이어 하나를 삭제해 파라미터의 갯수를 감소시켰다.

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%203.png)

- 성능은 앞에 모델보다 떨어졌으나 validation loss 와 training loss의 폭이 감소 했다.

### L2 Regularization

- 가중치의 제곱에 비례하는 노이즈를 Cost Function에 추가(가중치 감쇠:wight Decay)과대 조정을 방지한다

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%204.png)

- 

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%205.png)

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%206.png)

### Dropout

- 학습 과정에서 네트워크의 일부 연결을 무작위로 제외 시키고 이후 재구성하여 과적합을 막고 일반화 능력을 향상시킨다.
- 훈련 과정에서 네트워크의 복잡성을 줄이고, 테스트 데이터에서 더 나은 성능을 내도록 돕는다

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%207.png)

- dropout 적용

![Untitled](/image/Deep%20Neural%20Network%200cd2b12fbaa84302b219087d215c2dae/Untitled%208.png)

- 결과
- 앞서 했던 두방 법보다 정확도가 증가 하였다.

### Batch Normaliztion

- 활성화된 함수의 입력값에 대한 정규화를 수행
- Gradient Vanishing 문제 해결 및 더 큰 Learning Rate를 사용 가능
    - Gradient Vanishing 이란
        - 모델의 학습 과정에서 발생하는 중요한 현상
        - 층이 깊어질수록 Gradient의 크기가 급격히 줄어드는 현상
        - Gradient의 소실로 인해 최종 층의 가중치가 적절히 조절되지 못하고, 모델의 성능이 떨어지게 됨
- Batch Normaliztion
    - 네트워크의 각 레이어에서의 활성화 함수의 출력을 평균 0과 분산 1로 정규화 시킨다
    - 네트워크의 각 레이어에서의 활성화 함수의 출력 분포가 매우 유사하게 되고, 이후 레이어에서의 학습이 훨씬 더 안정적이고 효과적으로 이루어질 수 있게 된다.

        ![batch_normal](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/PYpzO/btqEbvPCvsc/3x9sukTLAwdqNWOkpwgTAk/img.png)